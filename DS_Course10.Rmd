---
title: "Data Science Capstone"
output: html_document
author: "Hoang Viet Tran"
date: "December 26, 2017"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Task 0: Understanding the Problem

### Task 1: Getting and Cleaning the Data

#### Download and unzip the dataset

``` {r getting_data}
data_link <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
file_zip <- "Coursera-SwiftKey.zip"
if (!file.exists(file_zip)) {
  download.file(data_link,file_zip,method="curl")
}
if (!dir.exists("final")) {
  unzip(file_zip)
}
```

#### Import datasets
```{r importing, cache=TRUE}
twitter <- readLines("final/en_US/en_US.twitter.txt",encoding="UTF-8",skipNul = T)
blogs <- readLines("final/en_US/en_US.blogs.txt",encoding="UTF-8",skipNul = T)
news <- readLines("final/en_US/en_US.news.txt",encoding ="UTF-8",skipNul = T)
```

#### Sample
I only select 5% data from each file for simplicity.

```{r sampling, cache=TRUE, dependson='importing', message=FALSE, warning=FALSE}
set.seed(48)
twitter.sample <- sample(twitter,length(twitter)*0.05,replace=FALSE)
blogs.sample <- sample(blogs,length(blogs)*0.05)
news.sample <- sample(news,length(news)*0.05)
```

Plot a histogram of number of words in each line

```{r histogram1, echo=F, dependson='sampling', message=FALSE, warning=FALSE}
library(stringi)
twitterWords <- stringi::stri_count_words(twitter.sample)
blogWords <- stringi::stri_count_words(blogs.sample)
newsWords <- stringi::stri_count_words(news.sample)
par(mfrow=c(1,3))
hist(twitterWords,breaks=15,col="orange",main="",xlab="Twitter")
hist(blogWords,breaks=30,col="orange",main="",xlab="Blogs")
hist(newsWords,breaks=20,col="orange",main="",xlab="News")
```

#### Data Cleaning
I combined all the three data files into a single file. I removed all numbers, special characters, bad words, and extra white space.

```{r cleaning, cache=TRUE, dependson='sampling'}
library(tm)
library(SnowballC)
new_data <- c(twitter.sample, blogs.sample, news.sample)
corpus <- Corpus(VectorSource(new_data))
remove.decimals <- function(x) {gsub("([0-9]*)\\.([0-9]+)", "\\1 \\2", x)}
remove.hashtags <- function(x) {gsub("#[a-zA-Z0-9]+", " ", x)}
remove.noneng <- function(x) {gsub("[^[:graph:]]", " ",x)}
corpus <- tm_map(corpus, remove.decimals)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, remove.noneng)
corpus <- tm_map(corpus, remove.hashtags)
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removePunctuation)

profanity <- read.csv("bad_words.txt", header = F)
profanity <- rep(profanity$V1)
corpus <- tm_map(corpus, removeWords, profanity)
```

### Task 2: Exploratory Analysis

#### Tokenization
I create one-gram tokenization by using RWeka library

```{r one_gram, cache=TRUE, dependson='cleaning'}
options(java.parameters="-Xmx8192m")
library(rJava)
library(RWeka)
oneGramToken <- NGramTokenizer(corpus,Weka_control(min=1,max=1))
oneGram <- data.frame(table(oneGramToken))
oneGramSorted <- oneGram[order(oneGram$Freq, decreasing = TRUE),]
saveRDS(oneGramToken, file="app/onegram.RData")
```

Here are the top 20 one-grams
```{r one_gram_list, dependson='one_gram'}
oneGramSorted[1:20,]
```

Two-gram tokenization
```{r two-gram, cache=TRUE, dependson='cleaning'}
options(java.parameters="-Xmx1092m")
twoGramToken <- NGramTokenizer(corpus,Weka_control(min=2,max=2))
twoGram <- data.frame(table(twoGramToken))
twoGramSorted <- twoGram[order(twoGram$Freq, decreasing = TRUE),]
saveRDS(twoGramToken, file="app/twogram.RData")
```

Here are the top 20 two-grams
```{r two-gram_list, dependson='two-gram', message=FALSE, warning=FALSE}
twoGramSorted[1:20,]
```
Three-gram tokenization
```{r three-gram, cache=TRUE, dependson='cleaning'}
options(java.parameters="-Xmx1092m")
threeGramToken <- NGramTokenizer(corpus,Weka_control(min=3,max=3))
threeGram <- data.frame(table(threeGramToken))
threeGramSorted <- threeGram[order(threeGram$Freq, decreasing = TRUE),]
saveRDS(threeGramToken, file="app/threegram.RData")
```

Here are the top 20 three-grams
```{r three-gram_list, dependson='three-gram', message=FALSE, warning=FALSE}
threeGramSorted[1:20,]
```

Four-gram tokenization
```{r four-gram, cache=TRUE, dependson='cleaning'}
options(java.parameters="-Xmx1092m")
fourGramToken <- NGramTokenizer(corpus,Weka_control(min=4,max=4))
fourGram <- data.frame(table(fourGramToken))
fourGramSorted <- fourGram[order(fourGram$Freq, decreasing = TRUE),]
saveRDS(fourGramToken, file="app/fourgram.RData")
```

Here are the top 20 four-grams
```{r four-gram_list, dependson='four-gram', message=FALSE, warning=FALSE}
fourGramSorted[1:20,]
```

Word Cloud
```{r word_cloud, cache=TRUE, message=FALSE, warning=FALSE}
library(wordcloud)
par(mfrow=c(1,2))
wordcloud(oneGramSorted[,1],freq=oneGramSorted[,2],scale=c(5,1),rot.per=F,min.freq=100,colors=brewer.pal(8,"Dark2"))
wordcloud(twoGramSorted[,1],freq=twoGramSorted[,2],scale=c(5,1),rot.per=F,min.freq=100,colors=brewer.pal(8,"Dark2"))
```

> Future Development

* Build a predictive model
* Build a shiny application



